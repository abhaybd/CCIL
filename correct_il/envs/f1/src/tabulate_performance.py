import argparse
import os
import re
import pickle
from typing import Dict, List
import numpy as np

from tabulate import tabulate

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("recordings", nargs="+", help="Path to recordings files generated by eval pipeline.")
    parser.add_argument("-s", "--save_plot", help="Path to save generated plots. Not generated if unspecified.")
    return parser.parse_args()

def get_algo_name(path: str):
    path = os.path.basename(path)[:-4]
    m = re.search(r"_seed\d+", path)
    if m:
        algo = path[:m.start()] + path[m.end():]
    else:
        algo = path
    return algo

def main():
    args = get_args()
    recs = args.recordings

    algo_map: Dict[str, List[str]] = {}

    for path in recs:
        algo = get_algo_name(path)
        if algo not in algo_map:
            algo_map[algo] = []
        algo_map[algo].append(path)
    
    table = [["Algo", "Avg Exp Ret", "Avg Exp Ret (std)", "Success Rate"]]
    label_names = {
        "bc_naive": "NaiveBC",
        "bc_noise.+": "NoiseBC",
        r".*[Ee]xpert.*": "expert",
        r"bc_\w+_\w+_[\d.]+": "CCIL",
        "milo": "MILO"
    }

    expert_rew = None
    for algo, rec_paths in algo_map.items():
        if re.match(r".*[Ee]xpert.*", algo):
            returns = []
            for rec_path in rec_paths:
                with open(rec_path, "rb") as f:
                    rec = pickle.load(f)
                    rews = []
                    for traj in rec["trajs"]:
                        rews.append(np.sum(traj["rewards"]))
                    returns.append(np.mean(rews))
            expert_rew = np.mean(returns)
    for algo, rec_paths in algo_map.items():
        label = algo
        for k, v in label_names.items():
            if re.match(k, algo):
                label = v
        row = [label]
        returns = []
        succs = []
        for rec_path in rec_paths:
            with open(rec_path, "rb") as f:
                rec = pickle.load(f)
                rews = []
                succ = 0
                for traj in rec["trajs"]:
                    rews.append(np.sum(traj["rewards"]))
                    if expert_rew:
                        rews[-1] /= expert_rew
                    if len(traj["rewards"]) == 800:
                        succ += 1
                returns.append(np.mean(rews))
                succs.append(succ / len(rec["trajs"]))
            print(rec_path, returns[-1], np.std(rews))
        row.append(np.mean(returns))
        row.append(np.std(returns))
        row.append(np.mean(succs))
        table.append(row)
    
    print(f"Tabular results across {len(next(iter(algo_map.values())))} seeds")
    print(tabulate(table, headers="firstrow", floatfmt=".3f", tablefmt="grid"))

    if args.save_plot:
        import matplotlib.pyplot as plt
        plt.rcParams.update({'font.size': 20})
        fig = plt.figure()
        ax = fig.add_subplot()
        if expert_rew:
            ax.axhline(1.0, ls="--", color="red", label="Expert")
            ax.legend(prop={"size": 12}, loc="upper right")
            table = [row for row in table if row[0] != "expert"]
        BASELINE = 0 if expert_rew else min(0.9*(row[1] - row[2]) for row in table[1:])
        ax.bar([row[0] for row in table[1:]], [row[1]-BASELINE for row in table[1:]], yerr=[row[2] for row in table[1:]], bottom=BASELINE)
        ax.set_ylabel("Normalized Expected Return")
        fig.tight_layout()
        fig.savefig(args.save_plot)

        fig = plt.figure()
        ax = fig.add_subplot()
        ax.bar([row[0] for row in table[1:]], [row[3] for row in table[1:]])
        ax.axhline(1.0, ls="--", color="red", label="Expert")
        ax.legend(prop={"size": 12}, loc="upper right")
        ax.set_ylabel("Success Rate")
        fig.tight_layout()
        save_pfx, ext = args.save_plot.rsplit(".", 1)
        fig.savefig(f"{save_pfx}_succ.{ext}")
    
if __name__ == "__main__":
    main()
